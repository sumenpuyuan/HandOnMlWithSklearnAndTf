{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多分类器投票 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anoconda\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_moons\n",
    "import numpy as np\n",
    "log_clf=LogisticRegression()\n",
    "rnd_clf=RandomForestClassifier()\n",
    "svm_clf=SVC()\n",
    "voting_clf=VotingClassifier(estimators=[('lr',log_clf),('rf',rnd_clf),('svc',svm_clf)],voting='hard')\n",
    "X, y = make_moons()\n",
    "from sklearn.cross_validation import train_test_split\n",
    "#x为数据集的feature熟悉，y为label.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "#voting_clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.8\n",
      "RandomForestClassifier 0.9333333333333333\n",
      "SVC 0.9\n",
      "VotingClassifier 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anoconda\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#测试准确率\n",
    "from sklearn.metrics import accuracy_score\n",
    "for clf in(log_clf,rnd_clf,svm_clf,voting_clf):\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    print(clf.__class__.__name__,accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging and Pasting #\n",
    "有放回采样时baggin，没放回时pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1]\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "#print(len(X_train))\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "bag_clf=BaggingClassifier(DecisionTreeClassifier(),n_estimators=500,max_samples=70,bootstrap=True,n_jobs=-1)\n",
    "bag_clf.fit(X_train,y_train)\n",
    "y_pred=bag_clf.predict(X_test)\n",
    "print(y_pred)\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 包外估计 #\n",
    "因为有30%的数据不会出现在训练集上，所以直接用oob来评价本身不需要在进行交叉验证和单独的验证机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9428571428571428\n"
     ]
    }
   ],
   "source": [
    "bag_clf=BaggingClassifier(DecisionTreeClassifier(),n_estimators=500,bootstrap=True,n_jobs=-1,oob_score=True)\n",
    "bag_clf.fit(X_train,y_train)\n",
    "print(bag_clf.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "y_pred=bag_clf.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机森林 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf=RandomForestClassifier(n_estimators=500,max_leaf_nodes=16,n_jobs=-1)\n",
    "rnd_clf.fit(X_train,y_train)\n",
    "y_pred=rnd_clf.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征重要度 #\n",
    "越靠近根部特征越重要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.1072391792533187\n",
      "sepal width (cm) 0.022558383346090005\n",
      "petal length (cm) 0.42940897177595744\n",
      "petal width (cm) 0.44079346562463373\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris=load_iris()\n",
    "rnd_clf=RandomForestClassifier(n_estimators=500,n_jobs=-1)\n",
    "rnd_clf.fit(iris['data'],iris['target'])\n",
    "for name,score in zip(iris['feature_names'],rnd_clf.feature_importances_):\n",
    "    print(name,score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adaboost #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_clf=AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=200,\n",
    "    algorithm='SAMME.R',\n",
    "    learning_rate=0.5\n",
    ")\n",
    "ada_clf.fit(X_train,y_train)\n",
    "y_pred=ada_clf.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  梯度提升 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg1=DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X_train,y_train)\n",
    "#在第一个分类器的残差上拟合第二个分类器\n",
    "y2=y_train-tree_reg1.predict(X_train)\n",
    "tree_reg2=DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X_train,y2)\n",
    "#在第二个分类器残差上拟合第三个分类器\n",
    "y3=y2-tree_reg2.predict(X_train)\n",
    "tree_reg3=DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X_train,y3)\n",
    "y_pred=sum(tree.predict(X_test) for tree in(tree_reg1,tree_reg2,tree_reg3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度提升决策树 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=1.0, loss='ls', max_depth=2, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=3, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbrt=GradientBoostingRegressor(max_depth=2,n_estimators=3,learning_rate=1.0)\n",
    "gbrt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了找到树的最优数量，你可以使用早停技术。最简单使用这个技术的方法就是使用stagged_predict()：他在训练的每一个阶段返回一个迭代器，接下来代码用120个树训练了一个gbdt集成，然后在训练的每个阶段验证错误以找到树的最佳数量，最后使用gbdt树的最有数量训练另一个集成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
